# ğŸ§  LLM Knowledge System (Hybrid RAG Search Engine)

A generic LLM knowledge system that integrates documents, web search, and multiple LLM providers with grounded, source-aware answers.

**Documents â€¢ Web â€¢ Hybrid Retrieval using LLMs**

An **open-source Hybrid Retrieval-Augmented Generation (RAG) system** built with **Streamlit, LangChain, FAISS, and modern LLM providers**.
This project enables users to upload documents, index them semantically, and ask questions grounded in **their private data**, optionally enriched with **real-time web search**.

> ğŸ“œ **License:** MIT â€” free to use, modify, and distribute.

---

## âœ¨ Features

* ğŸ“„ Upload multiple document types (`PDF`, `TXT`, `MD`)
* ğŸ” Semantic search using **FAISS vector store**
* ğŸ§  LLM-powered answers with **source attribution**
* ğŸŒ Optional real-time web search (Hybrid RAG)
* ğŸ’¬ Chat-based interface (Streamlit)
* ğŸ§± Modular, extensible architecture
* ğŸ§ª Designed for experimentation & learning

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UIâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Chat Interface       â”‚
â”‚  (Orchestrator)       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º Document Processing
       â”‚     (Load â†’ Chunk â†’ Embed)
       â”‚
       â”œâ”€â”€â–º Vector Store (FAISS)
       â”‚
       â”œâ”€â”€â–º Retriever
       â”‚     (Semantic / Hybrid)
       â”‚
       â””â”€â”€â–º LLM
             (Answer Generation)
```

---

## ğŸ§© Core Components Explained

### 1ï¸âƒ£ Streamlit UI (Frontend)

Responsible for:

* File upload
* Chat input/output
* Sidebar (uploaded files, controls)
* Feature toggles (web search, Wikipedia, etc.)

**Key principle:**
UI widgets are **stateless**; all persistence happens via controlled session state and backend services.

---

### 2ï¸âƒ£ Document Upload & Storage

**What happens when you upload a file:**

1. User uploads documents via `st.file_uploader`
2. Files are temporarily saved to `UPLOAD_DIR`
3. Only **filenames** are stored in session state
4. Actual file contents are processed and indexed

This avoids mixing:

* UI objects (`UploadedFile`)
* Persistent application state (`str`, metadata)

---

### 3ï¸âƒ£ Document Processing Pipeline

Each document goes through the following stages:

```
Raw File
  â†“
Document Loader
  â†“
Text Chunking
  â†“
Embedding Generation
  â†“
Vector Store Indexing
```

#### ğŸ”¹ Loaders

* PDF â†’ `PyPDFLoader`
* TXT / MD â†’ `TextLoader`

#### ğŸ”¹ Chunking

* Uses `RecursiveCharacterTextSplitter`
* Configurable `chunk_size` and `chunk_overlap`
* Preserves semantic boundaries

#### ğŸ”¹ Embeddings

* Powered by Hugging Face / Sentence Transformers
* Converts text chunks into dense vectors

---

### 4ï¸âƒ£ Vector Store (FAISS)

* Stores embeddings locally
* Enables fast similarity search
* Designed for **offline, private, low-latency retrieval**

Each chunk is stored with:

* Content
* Source filename
* Metadata (chunk index, type)

---

### 5ï¸âƒ£ Retriever Layer

Two modes are supported:

#### ğŸ“„ Document-Only RAG

* Searches only uploaded documents

#### ğŸŒ Hybrid RAG

* Combines:

  * Vector store results
  * Optional web search (Tavily / Wikipedia)

Retriever selects **most relevant context** before passing it to the LLM.

---

### 6ï¸âƒ£ LLM Layer

Responsible for:

* Understanding the user query
* Consuming retrieved context
* Generating a grounded answer
* Returning cited sources

The architecture supports:

* Groq `(Implemented)`
* OpenAI
* OpenRouter
* Gemini
* Any LangChain-compatible LLM

---

### 7ï¸âƒ£ Chat Memory & State

* Chat history is stored in `st.session_state`
* Each message contains:

  * Role (`user` / `assistant`)
  * Content
  * Sources (if available)

This enables:

* Conversational flow
* Context-aware follow-up questions

---

## ğŸ”„ End-to-End Flow

```
User uploads documents
        â†“
Documents saved temporarily
        â†“
Text chunked & embedded
        â†“
Embeddings stored in FAISS
        â†“
User asks a question
        â†“
Retriever fetches relevant context
        â†“
LLM generates grounded answer
        â†“
Answer + sources shown in chat
```

---

## ğŸ› ï¸ Tech Stack

| Layer         | Technology             |
| ------------- | ---------------------- |
| UI            | Streamlit              |
| LLM Framework | LangChain              |
| Vector DB     | FAISS                  |
| Embeddings    | Hugging Face           |
| LLM Providers | Groq / OpenAI / Gemini |
| Web Search    | Tavily / Wikipedia     |
| Language      | Python 3.10+           |

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/your-username/hybrid-rag-search-engine.git
cd hybrid-rag-search-engine
```

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # macOS / Linux
.venv\Scripts\activate     # Windows
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Configure Environment Variables

Create a `.env` file:

```env
LLM_PROVIDER=groq
GROQ_API_KEY=your_key_here
TAVILY_API_KEY=your_key_here
```

---

### 5ï¸âƒ£ Run the App

```bash
streamlit run app.py
```

---

## âš™ï¸ Configuration

All tunable parameters live in `config/settings.py`:

* Chunk size & overlap
* Upload directory
* Embedding model
* Vector store settings

---

## ğŸ§ª Use Cases

* Research paper Q&A
* Internal document search
* Knowledge base assistants
* Learning RAG systems
* Prototyping LLM applications

---

## ğŸ” Privacy & Security

* All uploaded documents stay **local**
* No data is sent externally unless:

  * Web search is explicitly enabled
* Vector store is stored locally

---

## ğŸ“¦ Extensibility

You can easily add:

* New document types
* New LLM providers
* Reranking models
* Persistent vector stores
* Authentication
* Multi-user support

---

## ğŸ¤ Contributing

Contributions are welcome!

* Fork the repo
* Create a feature branch
* Submit a PR with clear description

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
You are free to use, modify, and distribute it.

---

## ğŸ§  Final Note

This project is intentionally designed to be:

* **Readable**
* **Modular**
* **Educational**
* **Production-aligned**

If you understand this architecture, you understand **real-world RAG systems**.